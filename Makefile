# Motiflow - Makefile
# ===================
# Comprehensive build system for Motiflow project
# Supports web app, workflows, and infrastructure management

.DEFAULT_GOAL := help
.PHONY: help install dev build test clean start stop restart logs status format lint check deploy-local deploy-prod backup restore

# Project directories
WEB_DIR = web/motia-bridge
WORKFLOWS_DIR = workflows/motia-flows
INFRA_DIR = infra/docker

# Docker configuration
DOCKER_COMPOSE = docker compose -f $(INFRA_DIR)/docker-compose.yaml
DOCKER_ENV = $(INFRA_DIR)/.env

# Python optimization environment variables
export PYTHONDONTWRITEBYTECODE = 1
export PYTHONUNBUFFERED = 1
export UV_NO_CACHE = 0

# Colors for output
RED = \033[0;31m
GREEN = \033[0;32m
YELLOW = \033[0;33m
BLUE = \033[0;34m
PURPLE = \033[0;35m
CYAN = \033[0;36m
WHITE = \033[0;37m
NC = \033[0m # No Color

# Help target
help: ## ğŸ“š Show this help message
	@echo "$(CYAN)Motiflow Development Environment$(NC)"
	@echo "================================="
	@echo ""
	@echo "$(GREEN)Available targets:$(NC)"
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "  $(YELLOW)%-20s$(NC) %s\n", $$1, $$2}' $(MAKEFILE_LIST)
	@echo ""
	@echo "$(BLUE)Project Structure:$(NC)"
	@echo "  â€¢ web/motia-bridge/     - NextJS web application"
	@echo "  â€¢ workflows/motia-flows/ - Motia workflow engine"
	@echo "  â€¢ infra/docker/         - Docker infrastructure (MinIO + Spark)"

# =============================================================================
# Installation & Setup
# =============================================================================

install: ## ğŸ“¦ Install all dependencies (web + workflows + docker)
	@echo "$(GREEN)ğŸš€ Installing all project dependencies...$(NC)"
	@$(MAKE) install-web
	@$(MAKE) install-workflows
	@$(MAKE) install-infra
	@echo "$(GREEN)âœ… All dependencies installed successfully!$(NC)"

install-web: ## ğŸŒ Install web application dependencies
	@echo "$(BLUE)ğŸ“± Installing web dependencies...$(NC)"
	@cd $(WEB_DIR) && npm install
	@echo "$(GREEN)âœ… Web dependencies installed$(NC)"

install-workflows: ## âš™ï¸ Install workflow engine dependencies
	@echo "$(PURPLE)âš¡ Installing workflow dependencies...$(NC)"
	@cd $(WORKFLOWS_DIR) && npm install
	@echo "$(GREEN)âœ… Workflow dependencies installed$(NC)"

install-infra: ## ğŸ³ Setup Docker infrastructure
	@echo "$(CYAN)ğŸ³ Setting up Docker infrastructure...$(NC)"
	@echo "$(YELLOW)ğŸ” Checking uv availability...$(NC)"
	@if ! command -v uv >/dev/null 2>&1; then \
		echo "$(YELLOW)âš ï¸  uv not found. Installing uv...$(NC)"; \
		curl -LsSf https://astral.sh/uv/install.sh | sh; \
		echo "$(GREEN)âœ… uv installed$(NC)"; \
	else \
		echo "$(GREEN)âœ… uv is available$(NC)"; \
	fi
	@if [ ! -f $(DOCKER_ENV) ]; then \
		echo "$(YELLOW)âš ï¸  Creating .env file from template...$(NC)"; \
		if [ -f $(DOCKER_ENV).example ]; then \
			cp $(DOCKER_ENV).example $(DOCKER_ENV) && \
			sed -i '' '/# Copy this file to \.env and adjust values as needed/d' $(DOCKER_ENV); \
		else \
			echo "MINIO_USERNAME=minio\nMINIO_PASSWORD=minio123\nAWS_ACCESS_KEY_ID=minio\nAWS_SECRET_ACCESS_KEY=minio123\nMINIO_ACCESS_KEY=minio\nMINIO_SECRET_KEY=minio123" > $(DOCKER_ENV); \
		fi; \
	fi
	@echo "$(GREEN)âœ… Infrastructure setup complete$(NC)"

# =============================================================================
# Development
# =============================================================================

dev: ## ğŸš€ Start full development environment
	@echo "$(GREEN)ğŸš€ Starting full development environment...$(NC)"
	@$(MAKE) start-infra
	@$(MAKE) dev-parallel

dev-parallel: ## ğŸ”„ Run web and workflows in parallel development mode
	@echo "$(BLUE)ğŸ”„ Starting parallel development servers...$(NC)"
	@trap 'kill 0' INT; \
	(cd $(WEB_DIR) && npm run dev) & \
	(cd $(WORKFLOWS_DIR) && npm run dev) & \
	wait

dev-web: ## ğŸŒ Start web application in development mode
	@echo "$(BLUE)ğŸŒ Starting web development server...$(NC)"
	@cd $(WEB_DIR) && npm run dev

dev-workflows: ## âš™ï¸ Start workflow engine in development mode
	@echo "$(PURPLE)âš™ï¸ Starting workflow development server...$(NC)"
	@cd $(WORKFLOWS_DIR) && npm run dev

# =============================================================================
# Building
# =============================================================================

build: ## ğŸ—ï¸ Build all components
	@echo "$(GREEN)ğŸ—ï¸ Building all components...$(NC)"
	@$(MAKE) build-web
	@$(MAKE) build-workflows
	@$(MAKE) build-spark
	@echo "$(GREEN)âœ… All components built successfully!$(NC)"

build-web: ## ğŸŒ Build web application for production
	@echo "$(BLUE)ğŸŒ Building web application...$(NC)"
	@cd $(WEB_DIR) && npm run build
	@echo "$(GREEN)âœ… Web application built$(NC)"

build-workflows: ## âš™ï¸ Build workflow engine
	@echo "$(PURPLE)âš™ï¸ Building workflow engine...$(NC)"
	@cd $(WORKFLOWS_DIR) && npm run build
	@echo "$(GREEN)âœ… Workflow engine built$(NC)"

build-spark: ## âš¡ Build Spark Connect Docker image
	@echo "$(PURPLE)âš¡ Building Spark Connect Docker image...$(NC)"
	@$(DOCKER_COMPOSE) build spark-connect
	@echo "$(GREEN)âœ… Spark Connect image built$(NC)"

# =============================================================================
# Testing
# =============================================================================

test: ## ğŸ§ª Run all tests
	@echo "$(GREEN)ğŸ§ª Running all tests...$(NC)"
	@$(MAKE) test-web
	@$(MAKE) test-workflows
	@$(MAKE) test-spark-infra

test-web: ## ğŸŒ Run web application tests
	@echo "$(BLUE)ğŸŒ Running web tests...$(NC)"
	@cd $(WEB_DIR) && npm test 2>/dev/null || echo "$(YELLOW)âš ï¸  No tests configured for web app$(NC)"

test-workflows: ## âš™ï¸ Run workflow engine tests
	@echo "$(PURPLE)âš™ï¸ Running workflow tests...$(NC)"
	@cd $(WORKFLOWS_DIR) && npm test 2>/dev/null || echo "$(YELLOW)âš ï¸  No tests configured for workflows$(NC)"

test-spark-infra: ## âš¡ Test Spark Connect infrastructure
	@echo "$(PURPLE)âš¡ Testing Spark Connect infrastructure...$(NC)"
	@if [ ! -d "infra-testing/spark" ]; then \
		echo "$(RED)âŒ Spark infrastructure test directory not found$(NC)"; \
		exit 1; \
	fi
	@echo "$(YELLOW)ğŸ§ª Running Spark Connect tests with uv...$(NC)"
	@cd infra-testing/spark && \
	trap 'rm -rf .uv_tmp __pycache__ *.pyc 2>/dev/null || true' EXIT && \
	uv run --no-project main.py
	@echo "$(GREEN)âœ… Spark infrastructure tests completed$(NC)"

test-csv-datasets: ## ğŸ“Š Test CSV datasets with Spark Connect
	@echo "$(PURPLE)ğŸ“Š Testing CSV datasets with Spark Connect...$(NC)"
	@if [ ! -d "infra-testing/spark" ]; then \
		echo "$(RED)âŒ Spark infrastructure test directory not found$(NC)"; \
		exit 1; \
	fi
	@if [ ! -d "datasets-examples" ]; then \
		echo "$(RED)âŒ Examples directory not found$(NC)"; \
		exit 1; \
	fi
	@echo "$(YELLOW)ğŸ“ Testing CSV loading and analysis...$(NC)"
	@cd infra-testing/spark && \
	trap 'rm -rf .uv_tmp __pycache__ *.pyc 2>/dev/null || true' EXIT && \
	uv run --no-project test_csv_datasets.py
	@echo "$(GREEN)âœ… CSV dataset tests completed$(NC)"

test-spark-connect: ## âš¡ Quick Spark Connect connectivity test
	@echo "$(PURPLE)âš¡ Testing Spark Connect connectivity...$(NC)"
	@if ! nc -z localhost 15002 >/dev/null 2>&1; then \
		echo "$(RED)âŒ Spark Connect server not accessible on port 15002$(NC)"; \
		echo "$(YELLOW)ğŸ’¡ Run 'make start-spark' to start Spark Connect server$(NC)"; \
		exit 1; \
	fi
	@echo "$(YELLOW)â³ Port 15002 is open, checking if Spark Connect service is ready...$(NC)"
	@if $(DOCKER_COMPOSE) logs spark-connect 2>/dev/null | grep -q "Spark Connect server started at"; then \
		echo "$(GREEN)âœ… Spark Connect server is ready and accepting connections$(NC)"; \
	elif timeout 3s bash -c 'exec 3<>/dev/tcp/localhost/15002' 2>/dev/null; then \
		echo "$(GREEN)âœ… Spark Connect server is ready and accepting connections$(NC)"; \
	else \
		echo "$(YELLOW)âš ï¸  Port is open but Spark Connect may still be initializing$(NC)"; \
		echo "$(CYAN)ğŸ’¡ Check initialization status: 'make logs-spark'$(NC)"; \
		echo "$(CYAN)ğŸ’¡ Or test with actual Spark session: 'make test-spark-infra'$(NC)"; \
	fi

# =============================================================================
# Code Quality
# =============================================================================

lint: ## ğŸ” Lint all code
	@echo "$(GREEN)ğŸ” Linting all code...$(NC)"
	@$(MAKE) lint-web
	@$(MAKE) lint-workflows

lint-web: ## ğŸŒ Lint web application
	@echo "$(BLUE)ğŸŒ Linting web application...$(NC)"
	@cd $(WEB_DIR) && npm run lint

# FIXME: need to add a lint command
lint-workflows: ## âš™ï¸ Lint workflow engine
	@echo "$(PURPLE)âš™ï¸ Linting workflows...$(NC)"
	@cd $(WORKFLOWS_DIR) && npm run lint 2>/dev/null || echo "$(YELLOW)âš ï¸  No linting configured for workflows$(NC)"

format: ## âœ¨ Format all code
	@echo "$(GREEN)âœ¨ Formatting all code...$(NC)"
	@cd $(WEB_DIR) && npx prettier --write . 2>/dev/null || echo "$(YELLOW)âš ï¸  Prettier not configured$(NC)"
	@cd $(WORKFLOWS_DIR) && npx prettier --write . 2>/dev/null || echo "$(YELLOW)âš ï¸  Prettier not configured$(NC)"

check: ## ğŸ” Run all quality checks (lint + test)
	@$(MAKE) lint
	@$(MAKE) test

# =============================================================================
# Infrastructure Management
# =============================================================================

start-infra: ## ğŸ³ Start Docker infrastructure (MinIO, Spark, etc.)
	@echo "$(CYAN)ğŸ³ Starting Docker infrastructure...$(NC)"
	@$(DOCKER_COMPOSE) up -d --build
	@echo "$(GREEN)âœ… Infrastructure started$(NC)"
	@$(MAKE) wait-for-services

start-minio: ## ğŸ—„ï¸ Start only MinIO services
	@echo "$(CYAN)ğŸ—„ï¸ Starting MinIO services...$(NC)"
	@$(DOCKER_COMPOSE) up -d minio mc
	@echo "$(GREEN)âœ… MinIO services started$(NC)"

start-spark: ## âš¡ Start only Spark Connect services
	@echo "$(PURPLE)âš¡ Starting Spark Connect...$(NC)"
	@$(DOCKER_COMPOSE) up -d --build spark-connect
	@echo "$(GREEN)âœ… Spark Connect started$(NC)"
	@echo "$(YELLOW)ğŸ’¡ Spark Connect available at: spark://localhost:15002$(NC)"
	@echo "$(YELLOW)ğŸ’¡ Spark UI will be available at: http://localhost:4040-4045$(NC)"

start-spark-nobuild: ## âš¡ Start Spark Connect without building
	@echo "$(PURPLE)âš¡ Starting Spark Connect (no build)...$(NC)"
	@$(DOCKER_COMPOSE) up -d spark-connect
	@echo "$(GREEN)âœ… Spark Connect started$(NC)"

stop-infra: ## ğŸ›‘ Stop Docker infrastructure
	@echo "$(CYAN)ğŸ›‘ Stopping Docker infrastructure...$(NC)"
	@$(DOCKER_COMPOSE) down
	@echo "$(GREEN)âœ… Infrastructure stopped$(NC)"

stop-minio: ## ğŸ›‘ Stop MinIO services
	@echo "$(CYAN)ğŸ›‘ Stopping MinIO services...$(NC)"
	@$(DOCKER_COMPOSE) stop minio mc
	@echo "$(GREEN)âœ… MinIO services stopped$(NC)"

stop-spark: ## ğŸ›‘ Stop Spark Connect
	@echo "$(PURPLE)ğŸ›‘ Stopping Spark Connect...$(NC)"
	@$(DOCKER_COMPOSE) stop spark-connect
	@echo "$(GREEN)âœ… Spark Connect stopped$(NC)"

restart-infra: ## ğŸ”„ Restart Docker infrastructure
	@$(MAKE) stop-infra
	@$(MAKE) start-infra

restart-spark: ## ğŸ”„ Restart Spark cluster
	@$(MAKE) stop-spark
	@$(MAKE) start-spark

wait-for-services: ## â³ Wait for services to be ready
	@echo "$(YELLOW)â³ Waiting for services to be ready...$(NC)"
	@sleep 5
	@echo "$(YELLOW)â³ Checking MinIO...$(NC)"
	@for i in {1..30}; do \
		if curl -s http://localhost:9000/minio/health/live >/dev/null 2>&1; then \
			echo "$(GREEN)âœ… MinIO is ready!$(NC)"; \
			break; \
		fi; \
		echo "$(YELLOW)â³ Waiting for MinIO... ($$i/30)$(NC)"; \
		sleep 2; \
		if [ $$i -eq 30 ]; then \
			echo "$(RED)âŒ MinIO failed to start$(NC)"; \
			exit 1; \
		fi; \
	done
	@echo "$(YELLOW)â³ Checking Spark Connect...$(NC)"
	@echo "$(CYAN)ğŸ’¡ Spark Connect may take 2-5 minutes to download dependencies...$(NC)"
	@for i in {1..60}; do \
		if nc -z localhost 15002 >/dev/null 2>&1; then \
			echo "$(YELLOW)â³ Port 15002 is open, checking if Spark Connect is ready... ($$i/60)$(NC)"; \
			if $(DOCKER_COMPOSE) logs spark-connect 2>/dev/null | grep -q "Spark Connect server started at"; then \
				echo "$(GREEN)âœ… Spark Connect is ready and accepting connections!$(NC)"; \
				break; \
			elif timeout 3s bash -c 'exec 3<>/dev/tcp/localhost/15002' 2>/dev/null; then \
				echo "$(GREEN)âœ… Spark Connect is ready and accepting connections!$(NC)"; \
				break; \
			elif [ $$i -ge 45 ]; then \
				echo "$(YELLOW)âš ï¸  Spark Connect port is open but may still be initializing JARs$(NC)"; \
				echo "$(CYAN)ğŸ’¡ Check logs with 'make logs-spark' for download progress$(NC)"; \
				echo "$(GREEN)âœ… Infrastructure started - Spark Connect will be ready shortly$(NC)"; \
				break; \
			fi; \
		else \
			echo "$(YELLOW)â³ Waiting for Spark Connect to start... ($$i/60)$(NC)"; \
		fi; \
		sleep 3; \
		if [ $$i -eq 60 ]; then \
			echo "$(RED)âŒ Spark Connect failed to start within 3 minutes$(NC)"; \
			echo "$(CYAN)ğŸ’¡ Check logs with 'make logs-spark'$(NC)"; \
			exit 1; \
		fi; \
	done

# =============================================================================
# Production Management
# =============================================================================

start: ## ğŸš€ Start all services in production mode
	@echo "$(GREEN)ğŸš€ Starting all services...$(NC)"
	@$(MAKE) start-infra
	@$(MAKE) start-web-prod
	@$(MAKE) start-workflows-prod

start-web-prod: ## ğŸŒ Start web application in production mode
	@echo "$(BLUE)ğŸŒ Starting web in production mode...$(NC)"
	@cd $(WEB_DIR) && npm start

start-workflows-prod: ## âš™ï¸ Start workflow engine in production mode
	@echo "$(PURPLE)âš™ï¸ Starting workflows in production mode...$(NC)"
	@cd $(WORKFLOWS_DIR) && npm start 2>/dev/null || echo "$(YELLOW)âš ï¸  No production start script$(NC)"

stop: ## ğŸ›‘ Stop all services
	@echo "$(RED)ğŸ›‘ Stopping all services...$(NC)"
	@pkill -f "next start" 2>/dev/null || true
	@pkill -f "motia" 2>/dev/null || true
	@$(MAKE) stop-infra

restart: ## ğŸ”„ Restart all services
	@$(MAKE) stop
	@$(MAKE) start

# =============================================================================
# Monitoring & Logs
# =============================================================================
logs: ## ğŸ“‹ Show all infrastructure logs
	@$(DOCKER_COMPOSE) logs -f

logs-minio: ## ğŸ“‹ Show MinIO logs
	@$(DOCKER_COMPOSE) logs -f minio

logs-spark: ## ğŸ“‹ Show Spark Connect logs
	@$(DOCKER_COMPOSE) logs -f spark-connect

status: ## ğŸ“Š Show status of all services
	@echo "$(CYAN)ğŸ“Š Service Status$(NC)"
	@echo "=================="
	@echo "$(BLUE)ğŸ³ Docker Services:$(NC)"
	@$(DOCKER_COMPOSE) ps 2>/dev/null || echo "$(RED)âŒ Docker services not running$(NC)"
	@echo ""
	@echo "$(BLUE)ğŸŒ Web Application:$(NC)"
	@if pgrep -f "next" >/dev/null; then \
		echo "$(GREEN)âœ… NextJS running$(NC)"; \
	else \
		echo "$(RED)âŒ NextJS not running$(NC)"; \
	fi
	@echo ""
	@echo "$(BLUE)âš™ï¸ Workflow Engine:$(NC)"
	@if pgrep -f "motia" >/dev/null; then \
		echo "$(GREEN)âœ… Motia running$(NC)"; \
	else \
		echo "$(RED)âŒ Motia not running$(NC)"; \
	fi
	@echo ""
	@echo "$(BLUE)ğŸŒ Service Endpoints:$(NC)"
	@echo "  â€¢ Web App:        http://localhost:4000"
	@echo "  â€¢ Motia UI:       http://localhost:3000"
	@echo "  â€¢ MinIO API:      http://localhost:9000"
	@echo "  â€¢ MinIO UI:       http://localhost:9001"
	@echo "  â€¢ Spark Connect:  sc://localhost:15002"
	@echo "  â€¢ Spark UI:       http://localhost:4040-4045"

# =============================================================================
# Cleaning
# =============================================================================

clean: ## ğŸ§¹ Clean all build artifacts and dependencies
	@echo "$(RED)ğŸ§¹ Cleaning all build artifacts...$(NC)"
	@$(MAKE) clean-web
	@$(MAKE) clean-workflows
	@$(MAKE) clean-python
	@$(MAKE) clean-docker
	@echo "$(GREEN)âœ… All cleaned!$(NC)"

clean-web: ## ğŸŒ Clean web application
	@echo "$(BLUE)ğŸ§¹ Cleaning web application...$(NC)"
	@cd $(WEB_DIR) && rm -rf .next dist node_modules package-lock.json
	@echo "$(GREEN)âœ… Web cleaned$(NC)"

clean-workflows: ## âš™ï¸ Clean workflow engine
	@echo "$(PURPLE)ğŸ§¹ Cleaning workflow engine...$(NC)"
	@cd $(WORKFLOWS_DIR) && npm run clean 2>/dev/null || rm -rf dist node_modules .motia .mermaid package-lock.json
	@echo "$(GREEN)âœ… Workflows cleaned$(NC)"

clean-python: ## ğŸ Clean Python cache, temporary files and uv artifacts
	@echo "$(PURPLE)ğŸ§¹ Cleaning Python temporary files and caches...$(NC)"
	@find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	@find . -type f -name "*.pyc" -delete 2>/dev/null || true
	@find . -type f -name "*.pyo" -delete 2>/dev/null || true
	@find . -type d -name "*.egg-info" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name ".pytest_cache" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name ".uv_tmp" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name "venv" -path "*/infra-testing/*" -exec rm -rf {} + 2>/dev/null || true
	@uv cache clean 2>/dev/null || true
	@echo "$(GREEN)âœ… Python artifacts cleaned$(NC)"

clean-docker: ## ğŸ³ Clean Docker data and containers
	@echo "$(CYAN)ğŸ§¹ Cleaning Docker infrastructure...$(NC)"
	@$(DOCKER_COMPOSE) down -v --remove-orphans
	@docker system prune -f
	@echo "$(YELLOW)âš ï¸  Note: This will remove Spark cluster state and checkpoints$(NC)"
	@echo "$(GREEN)âœ… Docker cleaned$(NC)"

# =============================================================================
# Deployment
# =============================================================================

deploy-local: ## ğŸš€ Deploy locally (build + start)
	@echo "$(GREEN)ğŸš€ Deploying locally...$(NC)"
	@$(MAKE) build
	@$(MAKE) start

deploy-prod: ## ğŸŒ Deploy to production (placeholder)
	@echo "$(RED)ğŸŒ Production deployment not implemented yet$(NC)"
	@echo "$(YELLOW)TODO: Implement production deployment pipeline$(NC)"

# =============================================================================
# Utilities
# =============================================================================

update: ## ğŸ”„ Update all dependencies
	@echo "$(GREEN)ğŸ”„ Updating all dependencies...$(NC)"
	@cd $(WEB_DIR) && npm update
	@cd $(WORKFLOWS_DIR) && npm update
	@echo "$(GREEN)âœ… All dependencies updated$(NC)"

setup-dev: ## ğŸ› ï¸ Complete development environment setup
	@echo "$(GREEN)ğŸ› ï¸ Setting up complete development environment...$(NC)"
	@$(MAKE) clean
	@$(MAKE) install
	@$(MAKE) build
	@$(MAKE) start-infra
	@$(MAKE) cleanup-temp
	@echo "$(GREEN)âœ… Development environment ready! Run 'make dev' to start.$(NC)"

doctor: ## ğŸ” Check system health and requirements
	@echo "$(CYAN)ğŸ” System Health Check$(NC)"
	@echo "======================="
	@echo -n "$(BLUE)Node.js: $(NC)"
	@node --version 2>/dev/null || echo "$(RED)âŒ Not installed$(NC)"
	@echo -n "$(BLUE)npm: $(NC)"
	@npm --version 2>/dev/null || echo "$(RED)âŒ Not installed$(NC)"
	@echo -n "$(BLUE)Docker: $(NC)"
	@docker --version 2>/dev/null || echo "$(RED)âŒ Not installed$(NC)"
	@echo -n "$(BLUE)Docker Compose: $(NC)"
	@docker-compose --version 2>/dev/null || echo "$(RED)âŒ Not installed$(NC)"
	@echo -n "$(BLUE)Python3: $(NC)"
	@python3 --version 2>/dev/null || echo "$(RED)âŒ Not installed$(NC)"
	@echo -n "$(BLUE)uv (Python package manager): $(NC)"
	@uv --version 2>/dev/null || echo "$(RED)âŒ Not installed - install: curl -LsSf https://astral.sh/uv/install.sh | sh$(NC)"
	@echo ""
	@echo "$(BLUE)Project Dependencies:$(NC)"
	@if [ -d "$(WEB_DIR)/node_modules" ]; then echo "$(GREEN)âœ… Web dependencies$(NC)"; else echo "$(RED)âŒ Web dependencies - run 'make install-web'$(NC)"; fi
	@if [ -d "$(WORKFLOWS_DIR)/node_modules" ]; then echo "$(GREEN)âœ… Workflow dependencies$(NC)"; else echo "$(RED)âŒ Workflow dependencies - run 'make install-workflows'$(NC)"; fi
	@if [ -d "infra-testing/spark" ]; then echo "$(GREEN)âœ… Spark test environment$(NC)"; else echo "$(RED)âŒ Spark test environment - directory missing$(NC)"; fi
	@if [ -d "infra-testing/spark" ] && [ -f "infra-testing/spark/pyproject.toml" ]; then echo "$(GREEN)âœ… Spark test dependencies configured$(NC)"; else echo "$(YELLOW)âš ï¸  Spark test pyproject.toml - check infra-testing/spark/$(NC)"; fi
	@echo ""
	@echo "$(BLUE)Infrastructure Services:$(NC)"
	@if curl -s http://localhost:9000/minio/health/live >/dev/null 2>&1; then echo "$(GREEN)âœ… MinIO running$(NC)"; else echo "$(RED)âŒ MinIO not running - run 'make start-minio'$(NC)"; fi
	@if curl -s --connect-timeout 5 http://localhost:15002 >/dev/null 2>&1; then echo "$(GREEN)âœ… Spark Connect running$(NC)"; else echo "$(RED)âŒ Spark Connect not running - run 'make start-spark'$(NC)"; fi
	@echo ""
	@echo "$(BLUE)Quick Start Commands:$(NC)"
	@echo "  â€¢ Full setup:     make setup-dev"
	@echo "  â€¢ Start infra:    make start-infra"
	@echo "  â€¢ Test Spark:     make test-spark-infra"
	@echo "  â€¢ Clean Python:   make clean-python"
	@echo "  â€¢ Start dev:      make dev"

# =============================================================================
# Cleanup & Optimization
# =============================================================================

cleanup-temp: ## ğŸ§¹ Quick cleanup of temporary files (non-destructive)
	@echo "$(YELLOW)ğŸ§¹ Cleaning temporary files...$(NC)"
	@find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	@find . -type f -name "*.pyc" -delete 2>/dev/null || true
	@find . -type d -name ".uv_tmp" -exec rm -rf {} + 2>/dev/null || true
	@echo "$(GREEN)âœ… Temporary files cleaned$(NC)"

# =============================================================================
# Spark Connect Management  
# =============================================================================
spark-connect-shell: ## âš¡ Connect to Spark using PySpark shell
	@echo "$(PURPLE)âš¡ Opening PySpark shell connected to Spark Connect...$(NC)"
	@echo "$(YELLOW) Use: spark = SparkSession.builder.remote('sc://localhost:15002').getOrCreate()$(NC)"
	@cd infra-testing/spark && \
	trap 'rm -rf .uv_tmp __pycache__ *.pyc 2>/dev/null || true' EXIT && \
	uv run --no-project python3 -c "from pyspark.sql import SparkSession; print('ğŸš€ PySpark shell with uv - ready!'); import code; code.interact(local=locals())"

spark-connect-status: ## ğŸ“Š Check Spark Connect server status
	@echo "$(PURPLE)ğŸ“Š Spark Connect Status$(NC)"
	@echo "========================"
	@echo "$(BLUE)Checking Spark Connect server...$(NC)"
	@if ! nc -z localhost 15002 >/dev/null 2>&1; then \
		echo "$(RED)âŒ Spark Connect server not accessible on port 15002$(NC)"; \
		echo "$(YELLOW)ğŸ’¡ Run 'make start-spark' to start the server$(NC)"; \
	elif timeout 5s bash -c 'exec 3<>/dev/tcp/localhost/15002 && echo "test" >&3' 2>/dev/null; then \
		echo "$(GREEN)âœ… Spark Connect server is running and ready$(NC)"; \
	else \
		echo "$(YELLOW)âš ï¸  Port 15002 is open but Spark Connect may still be initializing$(NC)"; \
		echo "$(CYAN)ğŸ’¡ Checking recent logs for initialization progress...$(NC)"; \
		@$(DOCKER_COMPOSE) logs --tail=10 spark-connect 2>/dev/null | grep -E "(Started|Bound|Exception|Error)" || echo "$(YELLOW)Check full logs with 'make logs-spark'$(NC)"; \
	fi
	@echo ""
	@echo "$(BLUE)Docker Container Status:$(NC)"
	@$(DOCKER_COMPOSE) ps spark-connect 2>/dev/null || echo "$(RED)âŒ Spark Connect container not running$(NC)"
	@echo ""
	@echo "$(BLUE)MinIO Integration:$(NC)"
	@if curl -s http://localhost:9000/minio/health/live >/dev/null 2>&1; then \
		echo "$(GREEN)âœ… MinIO is running and accessible$(NC)"; \
	else \
		echo "$(RED)âŒ MinIO not accessible - run 'make start-minio'$(NC)"; \
	fi

spark-submit-test: ## âš¡ Submit our infrastructure test to Spark Connect
	@echo "$(PURPLE)âš¡ Submitting infrastructure test to Spark Connect...$(NC)"
	@$(MAKE) test-spark-infra

spark-connect-logs-follow: ## ğŸ“‹ Follow Spark Connect logs in real-time
	@echo "$(PURPLE)ğŸ“‹ Following Spark Connect logs...$(NC)"
	@$(DOCKER_COMPOSE) logs -f spark-connect

spark-wait-ready: ## â³ Wait for Spark Connect to be fully ready
	@echo "$(PURPLE)â³ Waiting for Spark Connect to be fully initialized...$(NC)"
	@echo "$(CYAN)ğŸ’¡ This may take 2-5 minutes for initial JAR downloads$(NC)"
	@for i in {1..100}; do \
		if nc -z localhost 15002 >/dev/null 2>&1; then \
			if $(DOCKER_COMPOSE) logs spark-connect 2>/dev/null | grep -q "Spark Connect server started at"; then \
				echo "$(GREEN)âœ… Spark Connect is fully ready!$(NC)"; \
				break; \
			elif timeout 3s bash -c 'exec 3<>/dev/tcp/localhost/15002' 2>/dev/null; then \
				echo "$(GREEN)âœ… Spark Connect is fully ready!$(NC)"; \
				break; \
			else \
				echo "$(YELLOW)â³ Spark Connect initializing... ($$i/100)$(NC)"; \
				if $(DOCKER_COMPOSE) logs --tail=3 spark-connect 2>/dev/null | grep -q "downloading\|SUCCESSFUL"; then \
					echo "$(CYAN)ğŸ“¦ Still downloading dependencies...$(NC)"; \
				fi; \
			fi; \
		else \
			echo "$(YELLOW)â³ Waiting for Spark Connect to start... ($$i/100)$(NC)"; \
		fi; \
		sleep 3; \
		if [ $$i -eq 100 ]; then \
			echo "$(RED)âŒ Spark Connect failed to become ready within 5 minutes$(NC)"; \
			echo "$(CYAN)ğŸ’¡ Check logs: 'make logs-spark'$(NC)"; \
			exit 1; \
		fi; \
	done

spark-connect-exec: ## ğŸ’» Execute command in Spark Connect container
	@echo "$(PURPLE)ğŸ’» Opening shell in Spark Connect container...$(NC)"
	@$(DOCKER_COMPOSE) exec spark-connect bash

spark-pyspark: ## âš¡ Open PySpark shell connected to Spark Connect
	@$(MAKE) spark-connect-shell

spark-status: ## ğŸ“Š Show Spark Connect status
	@$(MAKE) spark-connect-status

spark-rebuild: ## âš¡ Rebuild and restart Spark cluster
	@echo "$(PURPLE)âš¡ Rebuilding Spark cluster...$(NC)"
	@$(MAKE) stop-spark
	@$(MAKE) build-spark
	@$(MAKE) start-spark-nobuild
	@echo "$(GREEN)âœ… Spark cluster rebuilt and restarted$(NC)"

# =============================================================================
# Advanced
# =============================================================================

generate-types: ## ğŸ”§ Generate TypeScript types for workflows
	@echo "$(PURPLE)ğŸ”§ Generating TypeScript types...$(NC)"
	@cd $(WORKFLOWS_DIR) && npm run generate-types
	@echo "$(GREEN)âœ… Types generated$(NC)"

# =============================================================================
# Documentation Security (GPG)
# =============================================================================

lock-roadmap: ## ğŸ”’ Encrypt roadmap.md with GPG (requires passphrase)
	@echo "$(YELLOW)ğŸ”’ Encrypting roadmap.md with GPG...$(NC)"
	@if [ ! -f "docs/roadmap.md" ]; then \
		echo "$(RED)âŒ docs/roadmap.md not found$(NC)"; \
		echo "$(CYAN)ğŸ’¡ Use 'make unlock-roadmap' to decrypt first, or create the file$(NC)"; \
		exit 1; \
	fi
	@if command -v gpg >/dev/null 2>&1; then \
		echo "$(BLUE)ğŸ” You will be prompted for a passphrase...$(NC)"; \
		cd docs && gpg --symmetric --cipher-algo AES256 --output roadmap.md.gpg roadmap.md && \
		echo "$(GREEN)âœ… Roadmap encrypted successfully to docs/roadmap.md.gpg$(NC)" && \
		echo "$(YELLOW)ğŸ’¡ Original file preserved for local use$(NC)" && \
		echo "$(CYAN)ğŸ’¡ The encrypted file (.gpg) can be safely committed to git$(NC)" && \
		echo "$(CYAN)ğŸ’¡ Use 'make unlock-roadmap' to decrypt when needed$(NC)"; \
	else \
		echo "$(RED)âŒ GPG not installed$(NC)"; \
		echo "$(CYAN)ğŸ’¡ Install with: brew install gnupg$(NC)"; \
		exit 1; \
	fi

unlock-roadmap: ## ğŸ”“ Decrypt roadmap.md.gpg with GPG (requires passphrase)
	@echo "$(YELLOW)ğŸ”“ Decrypting roadmap.md.gpg with GPG...$(NC)"
	@if [ ! -f "docs/roadmap.md.gpg" ]; then \
		echo "$(RED)âŒ docs/roadmap.md.gpg not found$(NC)"; \
		echo "$(CYAN)ğŸ’¡ Use 'make lock-roadmap' to create encrypted version$(NC)"; \
		exit 1; \
	fi
	@if command -v gpg >/dev/null 2>&1; then \
		echo "$(BLUE)ğŸ” You will be prompted for the passphrase...$(NC)"; \
		cd docs && gpg --decrypt --output roadmap.md roadmap.md.gpg && \
		echo "$(GREEN)âœ… Roadmap decrypted successfully to docs/roadmap.md$(NC)" && \
		echo "$(YELLOW)âš ï¸  Remember: docs/roadmap.md is in .gitignore and won't be committed$(NC)" && \
		echo "$(CYAN)ğŸ’¡ Make your changes, then use 'make lock-roadmap' to update encrypted version$(NC)"; \
	else \
		echo "$(RED)âŒ GPG not installed$(NC)"; \
		echo "$(CYAN)ğŸ’¡ Install with: brew install gnupg$(NC)"; \
		exit 1; \
	fi

check-roadmap: ## ğŸ” Check roadmap status (encrypted vs decrypted)
	@echo "$(CYAN)ğŸ” Roadmap Status$(NC)"
	@echo "=================="
	@echo "$(BLUE)ğŸ“ Location: docs/$(NC)"
	@echo ""
	@if [ -f "docs/roadmap.md" ]; then \
		echo "$(GREEN)âœ… Decrypted version: docs/roadmap.md$(NC)"; \
		echo "   ğŸ“Š Size: $$(du -h docs/roadmap.md | cut -f1)"; \
		echo "   ğŸ“… Modified: $$(stat -f "%Sm" docs/roadmap.md)"; \
		echo "   ğŸ” Git status: Not tracked (in .gitignore)"; \
	else \
		echo "$(YELLOW)âš ï¸  No decrypted version found$(NC)"; \
		echo "   ğŸ’¡ Use 'make unlock-roadmap' to decrypt"; \
	fi
	@echo ""
	@if [ -f "docs/roadmap.md.gpg" ]; then \
		echo "$(GREEN)âœ… Encrypted version: docs/roadmap.md.gpg$(NC)"; \
		echo "   ğŸ“Š Size: $$(du -h docs/roadmap.md.gpg | cut -f1)"; \
		echo "   ğŸ“… Modified: $$(stat -f "%Sm" docs/roadmap.md.gpg)"; \
		echo "   ğŸ” Git status: Can be safely committed"; \
	else \
		echo "$(RED)âŒ No encrypted version found$(NC)"; \
		echo "   ğŸ’¡ Use 'make lock-roadmap' to create encrypted version"; \
	fi
	@echo ""
	@echo "$(BLUE)ğŸ”§ Available Commands:$(NC)"
	@echo "   ğŸ”“ Decrypt:  make unlock-roadmap"
	@echo "   ğŸ”’ Encrypt:  make lock-roadmap"
	@echo "   ğŸ” Status:   make check-roadmap"
	@echo ""
	@echo "$(BLUE)ğŸ” Security Notes:$(NC)"
	@echo "   â€¢ Encryption: AES256 symmetric encryption"
	@echo "   â€¢ Passphrase: Interactive prompt (secure)"
	@echo "   â€¢ Git: Only .gpg files are tracked"

clean-roadmap: ## ğŸ§¹ Remove decrypted roadmap (keep encrypted version)
	@echo "$(YELLOW)ğŸ§¹ Removing decrypted roadmap...$(NC)"
	@if [ -f "docs/roadmap.md" ]; then \
		rm docs/roadmap.md && \
		echo "$(GREEN)âœ… Decrypted version removed$(NC)" && \
		echo "$(CYAN)ğŸ’¡ Encrypted version preserved: docs/roadmap.md.gpg$(NC)"; \
	else \
		echo "$(YELLOW)âš ï¸  No decrypted version found$(NC)"; \
	fi

roadmap-workflow: ## ğŸ“‹ Show roadmap workflow guide
	@echo "$(CYAN)ğŸ“‹ Roadmap Workflow Guide$(NC)"
	@echo "=========================="
	@echo ""
	@echo "$(BLUE)ğŸš€ Getting Started:$(NC)"
	@echo "1. make unlock-roadmap    # Decrypt for editing"
	@echo "2. # Edit docs/roadmap.md"
	@echo "3. make lock-roadmap      # Encrypt changes"
	@echo "4. git add docs/roadmap.md.gpg"
	@echo "5. git commit -m 'Update roadmap'"
	@echo ""
	@echo "$(BLUE)ğŸ”„ Daily Workflow:$(NC)"
	@echo "â€¢ Morning:   make unlock-roadmap"
	@echo "â€¢ Work:      Edit docs/roadmap.md"
	@echo "â€¢ Evening:   make lock-roadmap"
	@echo "â€¢ Cleanup:   make clean-roadmap (optional)"
	@echo ""
	@echo "$(BLUE)ğŸ” Security Benefits:$(NC)"
	@echo "â€¢ Private planning in public repo"
	@echo "â€¢ AES256 encryption"
	@echo "â€¢ Personal passphrase protection"
	@echo "â€¢ No sensitive info in git history"

open-minio: ## ğŸŒ Open MinIO console in browser
	@echo "$(CYAN)ğŸŒ Opening MinIO console...$(NC)"
	@open http://localhost:9001 2>/dev/null || echo "Visit http://localhost:9001"

open-spark: ## ğŸŒ Open Spark UI in browser (when available)
	@echo "$(PURPLE)ğŸŒ Opening Spark UI...$(NC)"
	@echo "$(YELLOW)ğŸ’¡ Spark UI available when jobs are running: http://localhost:4040-4045$(NC)"
	@open http://localhost:4040 2>/dev/null || echo "Visit http://localhost:4040 when Spark jobs are running"

open-web: ## ğŸŒ Open web application in browser
	@echo "$(BLUE)ğŸŒ Opening web application...$(NC)"
	@open http://localhost:4000 2>/dev/null || echo "Visit http://localhost:4000"

open-workflows: ## ğŸŒ Open Motia UI for workflows
	@echo "$(PURPLE)ğŸŒ Opening Motia UI for workflows...$(NC)"
	@echo "$(YELLOW)ğŸ’¡ Motia UI available at: http://localhost:4000$(NC)"
	@open http://localhost:4000 2>/dev/null || echo "Visit http://localhost:4000"

shell-web: ## ğŸ’» Open shell in web container
	@echo "$(BLUE)ğŸ’» Opening shell in web directory...$(NC)"
	@cd $(WEB_DIR) && $(SHELL)

shell-workflows: ## ğŸ’» Open shell in workflows container
	@echo "$(PURPLE)ğŸ’» Opening shell in workflows directory...$(NC)"
	@cd $(WORKFLOWS_DIR) && $(SHELL)

# =============================================================================
# Quick Commands (aliases)
# =============================================================================

up: start-infra ## ğŸš€ Alias for start-infra
down: stop-infra ## ğŸ›‘ Alias for stop-infra
web: dev-web ## ğŸŒ Alias for dev-web
workflows: dev-workflows ## âš™ï¸ Alias for dev-workflows
pyspark: spark-connect-shell ## ğŸ Alias for spark-connect-shell
spark-ui: open-spark ## ğŸŒ Alias for open-spark
test-spark: test-spark-infra ## ğŸ§ª Alias for test-spark-infra
test-csv: test-csv-datasets ## ğŸ“Š Alias for test-csv-datasets
spark-logs: logs-spark ## ğŸ“‹ Alias for logs-spark
spark-shell: spark-connect-shell ## âš¡ Alias for spark-connect-shell
cleanup: cleanup-temp ## ğŸ§¹ Alias for cleanup-temp
clean-py: clean-python ## ğŸ Alias for clean-python

# Documentation aliases
roadmap: check-roadmap ## ğŸ“‹ Alias for check-roadmap
unlock: unlock-roadmap ## ğŸ”“ Alias for unlock-roadmap  
lock: lock-roadmap ## ğŸ”’ Alias for lock-roadmap
roadmap-help: roadmap-workflow ## ğŸ“š Alias for roadmap-workflow