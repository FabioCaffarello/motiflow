import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum as spark_sum


def create_spark_session():
    """Cria uma sessÃ£o Spark Connect"""
    spark = (
        SparkSession.builder.appName("MotiflowSparkConnectTest")
        .remote("sc://localhost:15002")
        .getOrCreate()
    )

    return spark


def test_basic_operations(spark):
    """Testa operaÃ§Ãµes bÃ¡sicas do Spark"""
    print("ğŸ”§ Testando operaÃ§Ãµes bÃ¡sicas do Spark...")

    # Criar um DataFrame simples
    data = [
        ("Alice", 25, "Engineering"),
        ("Bob", 30, "Marketing"),
        ("Charlie", 35, "Engineering"),
        ("Diana", 28, "Sales"),
        ("Eve", 32, "Engineering"),
    ]

    columns = ["name", "age", "department"]
    df = spark.createDataFrame(data, columns)

    print("ğŸ“Š DataFrame criado:")
    df.show()

    # Testar algumas operaÃ§Ãµes
    print("ğŸ‘¥ Contagem por departamento:")
    dept_count = (
        df.groupBy("department")
        .agg(count("*").alias("employee_count"), spark_sum("age").alias("total_age"))
        .orderBy("department")
    )

    dept_count.show()

    print("ğŸ¯ Filtro: FuncionÃ¡rios com mais de 30 anos:")
    older_employees = df.filter(col("age") > 30)
    older_employees.show()

    return df


def test_file_operations(spark):
    """Testa operaÃ§Ãµes com arquivos"""
    print("ğŸ“ Testando operaÃ§Ãµes com arquivos...")

    # Criar dados de teste
    test_data = [
        ("product_A", 100, 25.50),
        ("product_B", 200, 15.30),
        ("product_C", 150, 35.20),
        ("product_D", 75, 45.00),
        ("product_E", 300, 12.75),
    ]

    columns = ["product_name", "quantity", "price"]
    df = spark.createDataFrame(test_data, columns)

    # Salvar como Parquet (formato eficiente para Spark)
    output_path = "/tmp/test_data.parquet"
    print(f"ğŸ’¾ Salvando dados em: {output_path}")

    df.coalesce(1).write.mode("overwrite").parquet(output_path)

    # Ler os dados de volta
    print("ğŸ“– Lendo dados do arquivo:")
    df_read = spark.read.parquet(output_path)
    df_read.show()

    # Calcular algumas mÃ©tricas
    print("ğŸ“ˆ MÃ©tricas dos produtos:")
    metrics = df_read.agg(
        spark_sum("quantity").alias("total_quantity"),
        spark_sum(col("quantity") * col("price")).alias("total_revenue"),
        count("*").alias("product_count"),
    )

    metrics.show()

    return df_read


def test_pandas_integration(spark):
    """Testa integraÃ§Ã£o com Pandas"""
    print("ğŸ¼ Testando integraÃ§Ã£o Spark â†” Pandas...")

    # Criar DataFrame Pandas
    pandas_data = {
        "city": [
            "SÃ£o Paulo",
            "Rio de Janeiro",
            "Belo Horizonte",
            "Porto Alegre",
            "Recife",
        ],
        "population": [12_300_000, 6_700_000, 2_500_000, 1_500_000, 1_650_000],
        "area_km2": [1521, 1255, 331, 496, 218],
    }

    pandas_df = pd.DataFrame(pandas_data)
    print("ğŸ“Š DataFrame Pandas original:")
    print(pandas_df)

    # Converter para Spark DataFrame
    spark_df = spark.createDataFrame(pandas_df)

    # Adicionar coluna calculada
    spark_df = spark_df.withColumn(
        "density_per_km2", col("population") / col("area_km2")
    ).orderBy(col("density_per_km2").desc())

    print("\nğŸ™ï¸ Cidades ordenadas por densidade populacional:")
    spark_df.show()

    # Converter de volta para Pandas
    result_pandas = spark_df.toPandas()
    print("\nğŸ“Š Resultado convertido para Pandas:")
    print(result_pandas)

    return result_pandas


def test_minio_integration(spark):
    """Testa integraÃ§Ã£o com MinIO/S3"""
    print("â˜ï¸ Testando integraÃ§Ã£o com MinIO/S3...")

    try:
        # Criar dados de teste
        test_data = [
            ("transaction_001", "2024-01-01", 1500.00, "credit"),
            ("transaction_002", "2024-01-02", 800.50, "debit"),
            ("transaction_003", "2024-01-03", 2200.75, "credit"),
            ("transaction_004", "2024-01-04", 450.25, "debit"),
            ("transaction_005", "2024-01-05", 3000.00, "credit"),
        ]

        columns = ["transaction_id", "date", "amount", "type"]
        df = spark.createDataFrame(test_data, columns)

        print("ğŸ“Š Dados de transaÃ§Ã£o criados:")
        df.show()

        # Tentar salvar no MinIO (se estiver configurado)
        s3_path = "s3a://motiflow/test-data/transactions.parquet"
        print(f"ğŸ’¾ Tentando salvar no MinIO: {s3_path}")

        try:
            df.coalesce(1).write.mode("overwrite").parquet(s3_path)
            print("âœ… Dados salvos com sucesso no MinIO!")

            # Ler os dados de volta
            print("ğŸ“– Lendo dados do MinIO:")
            df_from_s3 = spark.read.parquet(s3_path)
            df_from_s3.show()

            # Calcular agregaÃ§Ãµes
            print("ğŸ“ˆ AnÃ¡lise das transaÃ§Ãµes:")
            analysis = (
                df_from_s3.groupBy("type")
                .agg(
                    count("*").alias("transaction_count"),
                    spark_sum("amount").alias("total_amount"),
                )
                .orderBy("type")
            )

            analysis.show()

            return True

        except Exception as e:
            print(
                f"âš ï¸ MinIO nÃ£o estÃ¡ configurado corretamente ou nÃ£o acessÃ­vel: {str(e)}"
            )
            print("ğŸ’¡ Continuando com testes locais...")
            return False

    except Exception as e:
        print(f"âŒ Erro no teste MinIO: {str(e)}")
        return False


def main():
    """FunÃ§Ã£o principal que executa todos os testes"""
    print("ğŸš€ Iniciando testes do Spark Connect...")
    print("=" * 50)

    try:
        # Criar sessÃ£o Spark
        spark = create_spark_session()
        print("âœ… ConexÃ£o com Spark Connect estabelecida!")
        print(f"ğŸ“‹ VersÃ£o do Spark: {spark.version}")
        print("ğŸ”§ ConfiguraÃ§Ã£o: MotiflowSparkConnectTest")
        print("=" * 50)

        # Executar testes
        test_basic_operations(spark)
        print("=" * 50)

        test_file_operations(spark)
        print("=" * 50)

        test_pandas_integration(spark)
        print("=" * 50)

        test_minio_integration(spark)
        print("=" * 50)

        print("ğŸ‰ Todos os testes passaram com sucesso!")
        print("âœ… Infraestrutura Spark Connect estÃ¡ funcionando corretamente!")

    except Exception as e:
        print(f"âŒ Erro durante os testes: {str(e)}")
        print("ğŸ” Verifique se o Spark Connect estÃ¡ rodando em localhost:15002")
        return 1

    finally:
        try:
            spark.stop()
            print("ğŸ›‘ SessÃ£o Spark encerrada.")
        except Exception:
            pass

    return 0


if __name__ == "__main__":
    exit_code = main()
